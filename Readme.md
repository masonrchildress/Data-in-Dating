### Problem Statement
Given the depth of the data gathered in this study, I will be attempting to accomplish two tasks. The first objective would be to create a classification model that will predict whether or not two people will be a match based on participant input. This could ultimately be used in dating apps as part of a recommender for potential partners. The second goal is to broadly perform EDA on the data to make either marketable or academic findings on people's preferences and ability to self identify.

### EDA
Initial EDA based mostly on correlations have revealed some interesting finds. Generally, the most important factors for a participant's partner to want a second date are in order: attractiveness, being fun, having shared interests, ambition, intelligence, then sincerity, with attractiveness and fun having more than twice any of the other traits. Interestingly enough, when split into men and women, intelligence helps men more than ambition, whereas for women ambition helps significantly more than ambition. A few other notable things that improved your odds were interests in clubbing, exercise, or sports, being caucasian, valuing/knowing that others value attractiveness, and going out/going out on dates more frequently. On the flipside, things that hurt your chances are dating/going out less, nerdier interests (education, gaming, engineering), saying ou value shared interests/sincerity, being asian, and the worst offender generally was you wanting to have a second date with them!

### Classification Models
In efforts to predict whether or not two people would match, I used a variety of features that revolve around participant input. These questions were meant to be superficial enough so as to not require objective measurements of people's traits nor any ongoing learning from the model. Features included: importance of same religion, importance of same race, rated interests in various hobbies, preferred traits in a date, what traits a date would prefer, and self ratings of those traits. Across both men and women, there were 84 features that I then performed a 2 level polynomial transformation on. After standardizing I ran the data through GridSearch using K-Nearest Neighbors, Logistic Regression, and Neural Networks. I achieved the best result using K-Nearest Neighbors with an accuracy of 85%. This sounds like a good result, however the baseline accuracy was 84%, so unfortunately the features I was testing did not seem to carry much signal.
